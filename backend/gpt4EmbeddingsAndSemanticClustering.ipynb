{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tiktoken langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAy2ajvrpy_c",
        "outputId": "5edc8675-f8b5-40a7-b923-a68c14958cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.203-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.9 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.10-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, aiohttp, openai, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.203 langchainplus-sdk-0.0.10 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 tiktoken-0.4.0 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOzSUu5k5HXn",
        "outputId": "f9e1347b-f696-4b1a-c05e-49168bae63fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "ZW_3qcw6qO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ''\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size):\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "    current_chunk = ''\n",
        "\n",
        "    for word in words:\n",
        "        if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "            current_chunk += word + ' '\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = word + ' '\n",
        "\n",
        "    chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "def extract_major_ideas(text):\n",
        "    major_ideas = []\n",
        "\n",
        "    max_iterations = 10\n",
        "    chunks = split_text_into_chunks(text, 4000)\n",
        "    conversation = [{'role': 'system', 'content': 'You are a helpful assistant that extracts key points from text.'}]\n",
        "    for chunk in chunks:\n",
        "        conversation.append({'role': 'user', 'content': chunk})\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-0613\",\n",
        "            messages=conversation,\n",
        "            max_tokens=1000,\n",
        "        )\n",
        "        user_message = response.choices[0].message['content']\n",
        "        major_ideas.append(user_message)\n",
        "        conversation = [{'role': 'system', 'content': 'You are a helpful assistant that extracts key points from text.'}]\n",
        "    return major_ideas"
      ],
      "metadata": {
        "id": "CZS1iLfaqMJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/calhacks/article.txt', 'r') as file:\n",
        "    large_text = file.read()"
      ],
      "metadata": {
        "id": "P8smTImK_Axa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "major_ideas = extract_major_ideas(large_text)"
      ],
      "metadata": {
        "id": "4RG1LRLM6QR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_major_ideas = []\n",
        "for major_idea in major_ideas:\n",
        "  for keypoint in major_idea.split(\"\\n\"):\n",
        "    all_major_ideas.append(keypoint[2:])\n",
        "all_major_ideas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S7KeBBjY_Dn",
        "outputId": "39a94ba6-b9d8-439e-b740-58c284db940c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Autonomous Driving (AD) systems rely on AI for safety and driving decisions, but these are vulnerable to adversarial attacks.',\n",
              " '\"Semantic AI security\" is the term referred to the research that explores the implications of these attacks not just at the component level, but at the system-wide level.',\n",
              " 'Over the past five years, research in this field has grown and is now being performed to address these challenges in AD.',\n",
              " 'This paper aims to systematically organise this emerging research field by analysing 53 papers. It categorises them based on aspects like the AI component targeted, the goal of the attack or defence, attack vectors, defence robustness, and evaluation methodologies.',\n",
              " 'Six significant scientific gaps are identified and the paper provides potential future directions for research and design.',\n",
              " 'An open source evaluation platform called PASS has been developed to address the gap in scientific methodology.',\n",
              " 'While AI components are vulnerable to attacks, these don\\'t necessarily result in system-level vulnerabilities due to the \"semantic gaps\" between system-level attack input and AI component level.',\n",
              " ' The text discusses the concept of semantic AI security, which addresses two general semantic gaps in artificial intelligence (AI). One is the system-to-AI semantic gap, also called the inverse-feature mapping problem. The second is the AI-to-system semantic gap, which involves mapping the impacts of AI component-level attacks to system-level impacts.',\n",
              " ' In the context of autonomous driving (AD), an exponentially growing trend in the research to tackle the semantic AI security challenges has been noticed since 2019.',\n",
              " ' Existing surveys on AD security did not focus on the semantic AI security challenges and their impacts on AD AI components.',\n",
              " ' Given the safety-criticality of attacks in the AD context and the increase in research on the topic, the authors believe it is a good time to summarize the current status, trends, scientific gaps, insights, and future research directions.',\n",
              " ' In total, 53 papers related to semantic AD AI security are collected and analyzed. These papers have been classified based on critical aspects for the security field like targeted AI component, attack/defence goals, vector, knowledge, deployability, robustness, and evaluation methodologies.',\n",
              " ' Based on the systematization, six significant scientific gaps were identified. The authors suggest potential future directions not only at the design level but also at the research goal and methodology levels.',\n",
              " 'is text discusses identified scientific gaps in autonomous vehicle (AD) AI security research, especially the general lack of system-level evaluation that could potentially lead to meaningless attack/defense progress. To fill this gap, the authors propose a community effort to build a common system-level evaluation infrastructure. They introduce their development of a uniform and extensible system-driven evaluation platform, named PASS, for the AD AI security research community, aiming to improve comparability, reproducibility, and to encourage open-sourcing. The platform has a simulation-centric hybrid design using both simulation and real vehicles. The authors also highlight the implementation of a prototype that system-level evaluates a popular AD AI attack category, proving the necessity of such an evaluation platform. The paper provides taxonomization of 53 related papers and analyses identifying the most substantial scientific gaps for future work.',\n",
              " ' In autonomous driving (AD), L1 vehicles have the AD system in control of either steering or throttling/braking, while L2 vehicles have partial automation, where the AD system controls both.',\n",
              " ' L1 and L2 vehicles still require active monitoring from the driver, but at levels L3 and above, driver attention is less necessary, and by levels L4 and L5, a driver seat is not required.',\n",
              " ' At L4 and L5 levels, the systems operate differently; L4 AD systems only operate in limited Operational Design Domains (ODDs), but L5 systems can handle all potential driving scenarios.',\n",
              " \" The AI components in AD systems include perception (understanding the surrounding environment), localization (finding the vehicle's position in the environment), prediction (estimating future statuses of surrounding objects), and planning (making driving decisions).\",\n",
              " ' The modular design is the current industry standard for AD systems due to its easier debugging, interpretation, and ability to hard-code safety rules/measures.',\n",
              " ' Autonomous vehicles are susceptible to adversarial AI attacks, in which AI models are manipulated, raising concerns about system-level vulnerabilities.',\n",
              " ' The paper focuses on exploring semantic AI security within the context of autonomous driving, with a view to addressing AI component-level vulnerabilities.',\n",
              " 'e key points from this text include:',\n",
              " '',\n",
              " ' The authors collect and evaluate research papers from top-tier venues and other well-known works in fields closely related to AD AI like security, Computer Vision (CV), Machine Learning (ML), AI, and robotics. They focus on research from 2017 to 2021.',\n",
              " ' Unlike previous reviews that focused on hardware and sensor security related to AD, this paper focuses on works involving semantic AD AI security, with an emphasis on research done later than 2019.',\n",
              " ' The authors look at the growth of semantic AD AI security work and argue that it forms an essential part of current research. Around 85% of this work has been developed since 2019.',\n",
              " ' The authors compare the AD AI research with similar security work on drones, automatic speech recognition and speaker identification, and sensor technology. ',\n",
              " ' The authors examine 53 papers focusing on semantic AD AI security, of which 48 discovered new attacks, and 8 developed new defense solutions.',\n",
              " ' Initially, researchers dismissed the severity of adversarial attacks on AD vehicles, but later works have proven this wrong, demonstrating successful attacks on object detection functions in cars.',\n",
              " \" The authors aim to categorize (or 'systematize') the existing research efforts related to AD AI. This includes highlighting targeted AI components in existing works, status, and trends.\",\n",
              " '',\n",
              " ' Majority of existing works (>86%) focus on perception, while localization, chassis, and end-to-end driving receive less or equal to 6.2% attention.',\n",
              " ' The two most popular perception works are camera (60.0%) and LiDAR (21.5%) perception.',\n",
              " ' None of the current works study downstream AI components such as prediction and planning.',\n",
              " ' The semantic AD AI attacks are categorized based on 3 research aspects: Attack goal, attack vector, and attacker’s knowledge.',\n",
              " ' Attack goals are further divided based on integrity, confidentiality, and availability. ',\n",
              " ' Integrity in AD context refers to the integrity of AI component outputs; its violations can lead to safety hazards, traffic rule violations, and mobility degradation.',\n",
              " ' Confidentiality is related to sensitive information from or collected by the AD vehicle.',\n",
              " \" Availability in the AD context can be defined as an AI component's ability to provide timely and reliable outputs; this can be affected through attacks causing delays or failures in the outputting function.\",\n",
              " ' Most existing works on AI components in autonomous vehicle systems focus is on integrity (96.3%), with only 3.7% on confidentiality, and none on availability.',\n",
              " ' Attacks on autonomous driving systems can be categorized into two groups: physical-layer and cyber-layer attacks.',\n",
              " ' Physical-layer attacks involve tampering with the sensor inputs to the AI components physically, which can be further broken down into physical-world attacks and sensor attacks.',\n",
              " ' Cyber-layer attacks require internal access to the autonomous driving system, its computation platform, or even its development environment.',\n",
              " ' The object texture, in physical-world attack vectors, refers to changing the surface texture of 2D or 3D objects, which is frequently used in adversarial attacks.',\n",
              " ' Multiple studies have been conducted in the field of AD AI attacks, targeting various aspects such as AI component integrity, object detection, camera perception, semantic segmentation, object tracking, LiDAR detection, and RADAR perception, among others. ',\n",
              " \" The understanding and execution of these attacks depend on the attacker's knowledge ranging from white-box knowledge (complete knowledge of the system), to gray-box (partial knowledge), and black-box (no knowledge).\",\n",
              " ' The attack methods often involve patches, posters, and software compromises.',\n",
              " \"e text discusses various methods that hackers can use to attack autonomous driving (AD) systems. Some of these attacks aim at disguising objects or changing their appearance through camouflage or projectors. Examples include altering the appearance of stop signs, road surfaces, vehicles, and clothes. Other attacks focus on changing the shape or position of 3D objects, such as vehicles or traffic cones. Sensor attacks involve spoofing devices like LiDAR, RADAR, and GPS to cause them to produce false readings. Another attack vector is projecting laser or light directly onto the sensor to misguide object detection. Acoustic signals can also be used to disrupt the outputs of Inertial Measurement Units (IMUs). Hackers can also use ML backdoor methods and software compromises to manipulate the model's outputs or to infiltrate sensor data. The majority of attacks use physical-world attack vectors, with object texture alteration being the most common approach. Cyber-layer attack vectors are less commonly used in current hacking approaches.\",\n",
              " 'The text discusses three attack settings on autonomous driving (AD) AI systems: white-box, gray-box, and black-box. In a white-box attack, the attacker has complete knowledge of the AD system; this is the most commonly adopted setting. Gray-box attacks assume that some information required for white-box attacks is unavailable, while black-box attacks are the most restrictive, with the attacker having no access to internal AD vehicle details. ',\n",
              " 'AD AI defense methods are categorized into two: consistency checking and adversarial robustness. Consistency checking cross-checks attacked information with other independent measurement sources or inherently unchanging properties of the information. Examples include using stereo cameras and prediction models to cross-check LiDAR object detection results, or checking if the current camera object detection results tally with the driving context. Adversarial robustness is another defense method but the text does not elaborate on this method.',\n",
              " 'The field has been evolving, with more studies focusing on gray-box and black-box settings, which are more challenging but practical.',\n",
              " 'Defense strategies are evaluated according to certain key factors: deployability, robustness to adaptive attacks, defense methods, and defense goals. The text provides a tabulated summary of various defense solutions, indicating whether they meet various evaluation criteria.',\n",
              " ' Robustness of the AI component against attacks is being improved through various defense methods such as adversarial training and predicting and removing potential adversarial perturbations.',\n",
              " ' There are two main defense goals; detection and mitigation. Detection-based defenses focus on detecting attack attempts while mitigation strategies aim to improve adversarial robustness to reduce attack success rate. ',\n",
              " ' Defense methods should be designed for practical deployment, focusing on aspects such as negligible timing overhead, negligible resource overhead, no model training, no additional dataset requirement, and no hardware modification. Current defenses lack awareness in areas like no model training and negligible resource overhead.',\n",
              " ' Adaptive attacks, designed to circumvent specific defenses, have become a point of focus and calls for defenses that can withstand such attacks. They assume complete knowledge of the defense internals and directly challenge the fundamental assumptions of the defense. Existing defenses have yet to be thoroughly evaluated against such attacks.',\n",
              " ' Adversarial AI defenses are strongly advocated and guidelines exist for designing adaptive attacks. ',\n",
              " ' Only 3 of the relevant studies include evaluations of adaptive attack (Nassi et al., Liu et al., and Sun et al.). ',\n",
              " \" Most existing defenses in AD AI security don't evaluate against adaptive attacks. \",\n",
              " ' Expected differences in evaluation methodologies exist due to different problem formulations. ',\n",
              " ' Evaluation methodology could be either at the level of the AI component or the AD system, involving both AI component and AD system.',\n",
              " ' Component-level evaluation looks at attack/defense impacts at the AI component level. ',\n",
              " ' System-level evaluation considers the impacts at the vehicle driving behavior level and can be achieved via real vehicle-based or simulation-based setups. ',\n",
              " ' Majority of surveyed works perform component-level evaluation while only about 25% adopt some form of system-level evaluation. ',\n",
              " ' There is a call for attention to areas of identified scientific gaps in AD AI security and suggestions for future directions.',\n",
              " ' In AD (Autonomous Driving) systems, there is a lack of system-level evaluation, with only 25.4% of existing works conducting this evaluation; especially low numbers (7.4%) are seen in the component of camera object detection.',\n",
              " ' Most existing works (74.6%) only carry out component-level evaluations without determining the system-level effects of their attack/defense designs.',\n",
              " ' This lack of system-level evaluation is falling behind other CPS (Cyber-Physical Systems) domains like drones and ASR/SI (automated speech recognition/speaker identification), which often conduct system-level evaluations.',\n",
              " ' AD systems are particularly complex, and the system-level complexity and stringent control dynamics can create fault-tolerant effects for component-level errors.',\n",
              " \" High attack success rates at the component-level may not necessarily affect the AD vehicle's driving behavior.\",\n",
              " ' The current lack of system-level evaluation in AD AI security research is a crucial scientific methodology-level gap that needs prompt attention.',\n",
              " ' Addressing this gap poses various technical challenges. Real vehicle-based system-level evaluation is generally unaffordable for most academic research groups and poses safety risks.',\n",
              " ' While simulation-based evaluation is more accessible and safe, it requires significant engineering efforts.',\n",
              " ' A potential solution could be a community-level effort to build a common system-level evaluation infrastructure, which could help address these challenges.',\n",
              " ' The system-level impact of attacks in Automated Driving (AD) can be significantly affected by various driving scenario setups.',\n",
              " ' The system-level evaluation results can only be compared if the same evaluation scenario and metric calculation methods are used.',\n",
              " ' Defense solutions for AD AI security attacks are quite limited today, particularly those focusing on attack prevention.',\n",
              " ' Among existing AD AI security studies, 85.7% are focused on discovered attacks, while only 14.3% are focused on effective defense solutions.',\n",
              " ' There are several AD AI components with discovered attacks but no effective defense solutions yet, indicating concrete areas for future research.',\n",
              " ' Generic AI defenses are generally ineffective against AD AI attacks, therefore new defense strategies need to be explored.',\n",
              " ' Existing works mostly focus on physical-layer attack vectors, leaving cyber-layer attack vectors relatively unexplored, with only 11.1% of AD AI attack studies relating to cyber threats.',\n",
              " ' In related Critical Physical Systems (CPS) domains such as drones and ASR/SI, the focus on cyber-layer attack vectors is much higher (>50%).',\n",
              " ' There is a need to address the system-to-AI semantic gap and potential deployability challenges for the successful implementation of attack prevention.',\n",
              " ' The general goal of Autonomous Driving (AD) AI stack is to achieve autonomy using onboard sensing, but there are networking channels that can affect end-to-end driving. ',\n",
              " ' High Definition (HD) Map update channels are crucial for driving safety as they are integral for various AD modules such as localization, prediction, routing, and planning.',\n",
              " ' Autonomous vehicles at Level 4 (L4) and above, which do not have safety drivers onboard, often require a remote operator to take over control when a failsafe state is reached. This control channel is directly safety-critical if hijacked.',\n",
              " ' Another area of concern is the vast majority of attack works targeting the upstream AI components like perception and localization, and not focusing much on the downstream components like prediction and planning.',\n",
              " ' The downstream AI components are just as important, if not more, than the upstream ones. For instance, errors in obstacle trajectory prediction or path planning will directly affect driving decisions, leading to system-level effects. ',\n",
              " ' In order to advance AD AI security research, there is a need to study the security properties of downstream AI components.',\n",
              " ' Possible solution directions include addressing the physical-layer attacks by manipulating road objects, affecting the inputs of prediction without relying on vulnerabilities in upstream components, and addressing physical-layer attacks by localization manipulation.',\n",
              " ' Changes in localization directly affect decision-making in driving path planning within autonomous driving (AD) systems. ',\n",
              " ' Attack vectors, such as GPS spoofing, can be used to manipulate planning inputs, but this depends on the type of localization system in use.',\n",
              " ' Cyber attacks are direct methods to manipulate inputs in AD systems and can include attacking a ROS node to send malicious messages. ',\n",
              " \" There's a focus on safety and rule violations in existing AD attacks, with less emphasis on other significant security aspects such as confidentiality, availability, and authenticity.\",\n",
              " \" There's a lack of balanced study between different aspects of security in the AD domain, as compared to drone security, where integrity, privacy, and availability are more equally studied.\",\n",
              " ' Future research should focus on exploring under-researched security properties, considering potential private information extraction from sensor inputs, and exploring more on cyber-layer attacks.',\n",
              " ' The openness of AD AI security works from the security community is lacking compared to other communities, which affects reproducibility and comparability.',\n",
              " ' Only a few papers from security conferences, specifically those related to sensor attacks, are open-source.',\n",
              " \" There's a need for more work on verifying the authenticity of safety drivers, passengers, and consumers in the AD AI stack.\",\n",
              " ' About 50% of ASR/SI papers in CPS, CV, and ML/AI security conferences release their code, highlighting a willingness to share, but the diversity of AD AI security papers may limit this practice.',\n",
              " ' Security conference papers tend to use a diverse set of attack vectors, making code sharing difficult due to hardware design implications.',\n",
              " ' Papers in the ASR/SI domain primarily use malicious sound waves for attacks, which are easier to modify and evaluate digitally.',\n",
              " ' Encouraging open-sourcing efforts within the security community is beneficial for future research, although it is unclear how hardware implementations should be shared to benefit the community directly.',\n",
              " \" Two potential solutions include open-source hardware implementation references and open-source attack modeling code, with the former involving the release of detailed hardware design information and the latter involving the digital modelling of an attack's capability.\",\n",
              " ' Open sourcing practices can be encouraged via community-level evaluation infrastructure development.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Embedding.create(\n",
        "    input=major_ideas,\n",
        "    model=\"text-similarity-babbage-001\"\n",
        ")"
      ],
      "metadata": {
        "id": "F0AU-fotXnZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = [ d['embedding'] for d in response['data']]\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "pc57mdGaXzfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_model = KMeans(n_clusters=10)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "print(cluster_assignment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDvJXSByX3wl",
        "outputId": "5449b8bd-8d1a-4ad6-bc3d-53fccead2e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8 2 8 1 2 9 7 5 4 3 0 0 0 1 0 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(major_ideas[sentence_id])\n",
        "clustered_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfyiCAu-X8Mn",
        "outputId": "bee56448-9620-4a14-9690-437fac3b3832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{8: ['- Autonomous Driving (AD) systems rely on AI for safety and driving decisions, but these are vulnerable to adversarial attacks.\\n- \"Semantic AI security\" is the term referred to the research that explores the implications of these attacks not just at the component level, but at the system-wide level.\\n- Over the past five years, research in this field has grown and is now being performed to address these challenges in AD.\\n- This paper aims to systematically organise this emerging research field by analysing 53 papers. It categorises them based on aspects like the AI component targeted, the goal of the attack or defence, attack vectors, defence robustness, and evaluation methodologies.\\n- Six significant scientific gaps are identified and the paper provides potential future directions for research and design.\\n- An open source evaluation platform called PASS has been developed to address the gap in scientific methodology.\\n- While AI components are vulnerable to attacks, these don\\'t necessarily result in system-level vulnerabilities due to the \"semantic gaps\" between system-level attack input and AI component level.',\n",
              "  'This text discusses identified scientific gaps in autonomous vehicle (AD) AI security research, especially the general lack of system-level evaluation that could potentially lead to meaningless attack/defense progress. To fill this gap, the authors propose a community effort to build a common system-level evaluation infrastructure. They introduce their development of a uniform and extensible system-driven evaluation platform, named PASS, for the AD AI security research community, aiming to improve comparability, reproducibility, and to encourage open-sourcing. The platform has a simulation-centric hybrid design using both simulation and real vehicles. The authors also highlight the implementation of a prototype that system-level evaluates a popular AD AI attack category, proving the necessity of such an evaluation platform. The paper provides taxonomization of 53 related papers and analyses identifying the most substantial scientific gaps for future work.'],\n",
              " 2: ['1. The text discusses the concept of semantic AI security, which addresses two general semantic gaps in artificial intelligence (AI). One is the system-to-AI semantic gap, also called the inverse-feature mapping problem. The second is the AI-to-system semantic gap, which involves mapping the impacts of AI component-level attacks to system-level impacts.\\n2. In the context of autonomous driving (AD), an exponentially growing trend in the research to tackle the semantic AI security challenges has been noticed since 2019.\\n3. Existing surveys on AD security did not focus on the semantic AI security challenges and their impacts on AD AI components.\\n4. Given the safety-criticality of attacks in the AD context and the increase in research on the topic, the authors believe it is a good time to summarize the current status, trends, scientific gaps, insights, and future research directions.\\n5. In total, 53 papers related to semantic AD AI security are collected and analyzed. These papers have been classified based on critical aspects for the security field like targeted AI component, attack/defence goals, vector, knowledge, deployability, robustness, and evaluation methodologies.\\n6. Based on the systematization, six significant scientific gaps were identified. The authors suggest potential future directions not only at the design level but also at the research goal and methodology levels.',\n",
              "  \"The key points from this text include:\\n\\n1. The authors collect and evaluate research papers from top-tier venues and other well-known works in fields closely related to AD AI like security, Computer Vision (CV), Machine Learning (ML), AI, and robotics. They focus on research from 2017 to 2021.\\n2. Unlike previous reviews that focused on hardware and sensor security related to AD, this paper focuses on works involving semantic AD AI security, with an emphasis on research done later than 2019.\\n3. The authors look at the growth of semantic AD AI security work and argue that it forms an essential part of current research. Around 85% of this work has been developed since 2019.\\n4. The authors compare the AD AI research with similar security work on drones, automatic speech recognition and speaker identification, and sensor technology. \\n5. The authors examine 53 papers focusing on semantic AD AI security, of which 48 discovered new attacks, and 8 developed new defense solutions.\\n6. Initially, researchers dismissed the severity of adversarial attacks on AD vehicles, but later works have proven this wrong, demonstrating successful attacks on object detection functions in cars.\\n7. The authors aim to categorize (or 'systematize') the existing research efforts related to AD AI. This includes highlighting targeted AI components in existing works, status, and trends.\\n\"],\n",
              " 1: [\"1. In autonomous driving (AD), L1 vehicles have the AD system in control of either steering or throttling/braking, while L2 vehicles have partial automation, where the AD system controls both.\\n2. L1 and L2 vehicles still require active monitoring from the driver, but at levels L3 and above, driver attention is less necessary, and by levels L4 and L5, a driver seat is not required.\\n3. At L4 and L5 levels, the systems operate differently; L4 AD systems only operate in limited Operational Design Domains (ODDs), but L5 systems can handle all potential driving scenarios.\\n4. The AI components in AD systems include perception (understanding the surrounding environment), localization (finding the vehicle's position in the environment), prediction (estimating future statuses of surrounding objects), and planning (making driving decisions).\\n5. The modular design is the current industry standard for AD systems due to its easier debugging, interpretation, and ability to hard-code safety rules/measures.\\n6. Autonomous vehicles are susceptible to adversarial AI attacks, in which AI models are manipulated, raising concerns about system-level vulnerabilities.\\n7. The paper focuses on exploring semantic AI security within the context of autonomous driving, with a view to addressing AI component-level vulnerabilities.\",\n",
              "  '1. The general goal of Autonomous Driving (AD) AI stack is to achieve autonomy using onboard sensing, but there are networking channels that can affect end-to-end driving. \\n2. High Definition (HD) Map update channels are crucial for driving safety as they are integral for various AD modules such as localization, prediction, routing, and planning.\\n3. Autonomous vehicles at Level 4 (L4) and above, which do not have safety drivers onboard, often require a remote operator to take over control when a failsafe state is reached. This control channel is directly safety-critical if hijacked.\\n4. Another area of concern is the vast majority of attack works targeting the upstream AI components like perception and localization, and not focusing much on the downstream components like prediction and planning.\\n5. The downstream AI components are just as important, if not more, than the upstream ones. For instance, errors in obstacle trajectory prediction or path planning will directly affect driving decisions, leading to system-level effects. \\n6. In order to advance AD AI security research, there is a need to study the security properties of downstream AI components.\\n7. Possible solution directions include addressing the physical-layer attacks by manipulating road objects, affecting the inputs of prediction without relying on vulnerabilities in upstream components, and addressing physical-layer attacks by localization manipulation.'],\n",
              " 9: [\"1. Majority of existing works (>86%) focus on perception, while localization, chassis, and end-to-end driving receive less or equal to 6.2% attention.\\n2. The two most popular perception works are camera (60.0%) and LiDAR (21.5%) perception.\\n3. None of the current works study downstream AI components such as prediction and planning.\\n4. The semantic AD AI attacks are categorized based on 3 research aspects: Attack goal, attack vector, and attacker’s knowledge.\\n5. Attack goals are further divided based on integrity, confidentiality, and availability. \\n6. Integrity in AD context refers to the integrity of AI component outputs; its violations can lead to safety hazards, traffic rule violations, and mobility degradation.\\n7. Confidentiality is related to sensitive information from or collected by the AD vehicle.\\n8. Availability in the AD context can be defined as an AI component's ability to provide timely and reliable outputs; this can be affected through attacks causing delays or failures in the outputting function.\"],\n",
              " 7: [\"1. Most existing works on AI components in autonomous vehicle systems focus is on integrity (96.3%), with only 3.7% on confidentiality, and none on availability.\\n2. Attacks on autonomous driving systems can be categorized into two groups: physical-layer and cyber-layer attacks.\\n3. Physical-layer attacks involve tampering with the sensor inputs to the AI components physically, which can be further broken down into physical-world attacks and sensor attacks.\\n4. Cyber-layer attacks require internal access to the autonomous driving system, its computation platform, or even its development environment.\\n5. The object texture, in physical-world attack vectors, refers to changing the surface texture of 2D or 3D objects, which is frequently used in adversarial attacks.\\n6. Multiple studies have been conducted in the field of AD AI attacks, targeting various aspects such as AI component integrity, object detection, camera perception, semantic segmentation, object tracking, LiDAR detection, and RADAR perception, among others. \\n7. The understanding and execution of these attacks depend on the attacker's knowledge ranging from white-box knowledge (complete knowledge of the system), to gray-box (partial knowledge), and black-box (no knowledge).\\n8. The attack methods often involve patches, posters, and software compromises.\"],\n",
              " 5: [\"The text discusses various methods that hackers can use to attack autonomous driving (AD) systems. Some of these attacks aim at disguising objects or changing their appearance through camouflage or projectors. Examples include altering the appearance of stop signs, road surfaces, vehicles, and clothes. Other attacks focus on changing the shape or position of 3D objects, such as vehicles or traffic cones. Sensor attacks involve spoofing devices like LiDAR, RADAR, and GPS to cause them to produce false readings. Another attack vector is projecting laser or light directly onto the sensor to misguide object detection. Acoustic signals can also be used to disrupt the outputs of Inertial Measurement Units (IMUs). Hackers can also use ML backdoor methods and software compromises to manipulate the model's outputs or to infiltrate sensor data. The majority of attacks use physical-world attack vectors, with object texture alteration being the most common approach. Cyber-layer attack vectors are less commonly used in current hacking approaches.\"],\n",
              " 4: ['- The text discusses three attack settings on autonomous driving (AD) AI systems: white-box, gray-box, and black-box. In a white-box attack, the attacker has complete knowledge of the AD system; this is the most commonly adopted setting. Gray-box attacks assume that some information required for white-box attacks is unavailable, while black-box attacks are the most restrictive, with the attacker having no access to internal AD vehicle details. \\n- AD AI defense methods are categorized into two: consistency checking and adversarial robustness. Consistency checking cross-checks attacked information with other independent measurement sources or inherently unchanging properties of the information. Examples include using stereo cameras and prediction models to cross-check LiDAR object detection results, or checking if the current camera object detection results tally with the driving context. Adversarial robustness is another defense method but the text does not elaborate on this method.\\n- The field has been evolving, with more studies focusing on gray-box and black-box settings, which are more challenging but practical.\\n- Defense strategies are evaluated according to certain key factors: deployability, robustness to adaptive attacks, defense methods, and defense goals. The text provides a tabulated summary of various defense solutions, indicating whether they meet various evaluation criteria.'],\n",
              " 3: ['1. Robustness of the AI component against attacks is being improved through various defense methods such as adversarial training and predicting and removing potential adversarial perturbations.\\n2. There are two main defense goals; detection and mitigation. Detection-based defenses focus on detecting attack attempts while mitigation strategies aim to improve adversarial robustness to reduce attack success rate. \\n3. Defense methods should be designed for practical deployment, focusing on aspects such as negligible timing overhead, negligible resource overhead, no model training, no additional dataset requirement, and no hardware modification. Current defenses lack awareness in areas like no model training and negligible resource overhead.\\n4. Adaptive attacks, designed to circumvent specific defenses, have become a point of focus and calls for defenses that can withstand such attacks. They assume complete knowledge of the defense internals and directly challenge the fundamental assumptions of the defense. Existing defenses have yet to be thoroughly evaluated against such attacks.'],\n",
              " 0: [\"1. Adversarial AI defenses are strongly advocated and guidelines exist for designing adaptive attacks. \\n2. Only 3 of the relevant studies include evaluations of adaptive attack (Nassi et al., Liu et al., and Sun et al.). \\n3. Most existing defenses in AD AI security don't evaluate against adaptive attacks. \\n4. Expected differences in evaluation methodologies exist due to different problem formulations. \\n5. Evaluation methodology could be either at the level of the AI component or the AD system, involving both AI component and AD system.\\n6. Component-level evaluation looks at attack/defense impacts at the AI component level. \\n7. System-level evaluation considers the impacts at the vehicle driving behavior level and can be achieved via real vehicle-based or simulation-based setups. \\n8. Majority of surveyed works perform component-level evaluation while only about 25% adopt some form of system-level evaluation. \\n9. There is a call for attention to areas of identified scientific gaps in AD AI security and suggestions for future directions.\",\n",
              "  \"1. In AD (Autonomous Driving) systems, there is a lack of system-level evaluation, with only 25.4% of existing works conducting this evaluation; especially low numbers (7.4%) are seen in the component of camera object detection.\\n2. Most existing works (74.6%) only carry out component-level evaluations without determining the system-level effects of their attack/defense designs.\\n3. This lack of system-level evaluation is falling behind other CPS (Cyber-Physical Systems) domains like drones and ASR/SI (automated speech recognition/speaker identification), which often conduct system-level evaluations.\\n4. AD systems are particularly complex, and the system-level complexity and stringent control dynamics can create fault-tolerant effects for component-level errors.\\n5. High attack success rates at the component-level may not necessarily affect the AD vehicle's driving behavior.\\n6. The current lack of system-level evaluation in AD AI security research is a crucial scientific methodology-level gap that needs prompt attention.\\n7. Addressing this gap poses various technical challenges. Real vehicle-based system-level evaluation is generally unaffordable for most academic research groups and poses safety risks.\\n8. While simulation-based evaluation is more accessible and safe, it requires significant engineering efforts.\\n9. A potential solution could be a community-level effort to build a common system-level evaluation infrastructure, which could help address these challenges.\",\n",
              "  '1. The system-level impact of attacks in Automated Driving (AD) can be significantly affected by various driving scenario setups.\\n2. The system-level evaluation results can only be compared if the same evaluation scenario and metric calculation methods are used.\\n3. Defense solutions for AD AI security attacks are quite limited today, particularly those focusing on attack prevention.\\n4. Among existing AD AI security studies, 85.7% are focused on discovered attacks, while only 14.3% are focused on effective defense solutions.\\n5. There are several AD AI components with discovered attacks but no effective defense solutions yet, indicating concrete areas for future research.\\n6. Generic AI defenses are generally ineffective against AD AI attacks, therefore new defense strategies need to be explored.\\n7. Existing works mostly focus on physical-layer attack vectors, leaving cyber-layer attack vectors relatively unexplored, with only 11.1% of AD AI attack studies relating to cyber threats.\\n8. In related Critical Physical Systems (CPS) domains such as drones and ASR/SI, the focus on cyber-layer attack vectors is much higher (>50%).\\n9. There is a need to address the system-to-AI semantic gap and potential deployability challenges for the successful implementation of attack prevention.',\n",
              "  \"1. Changes in localization directly affect decision-making in driving path planning within autonomous driving (AD) systems. \\n2. Attack vectors, such as GPS spoofing, can be used to manipulate planning inputs, but this depends on the type of localization system in use.\\n3. Cyber attacks are direct methods to manipulate inputs in AD systems and can include attacking a ROS node to send malicious messages. \\n4. There's a focus on safety and rule violations in existing AD attacks, with less emphasis on other significant security aspects such as confidentiality, availability, and authenticity.\\n5. There's a lack of balanced study between different aspects of security in the AD domain, as compared to drone security, where integrity, privacy, and availability are more equally studied.\\n6. Future research should focus on exploring under-researched security properties, considering potential private information extraction from sensor inputs, and exploring more on cyber-layer attacks.\\n7. The openness of AD AI security works from the security community is lacking compared to other communities, which affects reproducibility and comparability.\\n8. Only a few papers from security conferences, specifically those related to sensor attacks, are open-source.\\n9. There's a need for more work on verifying the authenticity of safety drivers, passengers, and consumers in the AD AI stack.\"],\n",
              " 6: [\"1. About 50% of ASR/SI papers in CPS, CV, and ML/AI security conferences release their code, highlighting a willingness to share, but the diversity of AD AI security papers may limit this practice.\\n2. Security conference papers tend to use a diverse set of attack vectors, making code sharing difficult due to hardware design implications.\\n3. Papers in the ASR/SI domain primarily use malicious sound waves for attacks, which are easier to modify and evaluate digitally.\\n4. Encouraging open-sourcing efforts within the security community is beneficial for future research, although it is unclear how hardware implementations should be shared to benefit the community directly.\\n5. Two potential solutions include open-source hardware implementation references and open-source attack modeling code, with the former involving the release of detailed hardware design information and the latter involving the digital modelling of an attack's capability.\\n6. Open sourcing practices can be encouraged via community-level evaluation infrastructure development.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clustered_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsP8JNN5m5l7",
        "outputId": "165f571a-701a-4b33-c155-df0ad4288e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{8: ['- Autonomous Driving (AD) systems rely on AI for safety and driving decisions, but these are vulnerable to adversarial attacks.\\n- \"Semantic AI security\" is the term referred to the research that explores the implications of these attacks not just at the component level, but at the system-wide level.\\n- Over the past five years, research in this field has grown and is now being performed to address these challenges in AD.\\n- This paper aims to systematically organise this emerging research field by analysing 53 papers. It categorises them based on aspects like the AI component targeted, the goal of the attack or defence, attack vectors, defence robustness, and evaluation methodologies.\\n- Six significant scientific gaps are identified and the paper provides potential future directions for research and design.\\n- An open source evaluation platform called PASS has been developed to address the gap in scientific methodology.\\n- While AI components are vulnerable to attacks, these don\\'t necessarily result in system-level vulnerabilities due to the \"semantic gaps\" between system-level attack input and AI component level.',\n",
              "  'This text discusses identified scientific gaps in autonomous vehicle (AD) AI security research, especially the general lack of system-level evaluation that could potentially lead to meaningless attack/defense progress. To fill this gap, the authors propose a community effort to build a common system-level evaluation infrastructure. They introduce their development of a uniform and extensible system-driven evaluation platform, named PASS, for the AD AI security research community, aiming to improve comparability, reproducibility, and to encourage open-sourcing. The platform has a simulation-centric hybrid design using both simulation and real vehicles. The authors also highlight the implementation of a prototype that system-level evaluates a popular AD AI attack category, proving the necessity of such an evaluation platform. The paper provides taxonomization of 53 related papers and analyses identifying the most substantial scientific gaps for future work.'],\n",
              " 2: ['1. The text discusses the concept of semantic AI security, which addresses two general semantic gaps in artificial intelligence (AI). One is the system-to-AI semantic gap, also called the inverse-feature mapping problem. The second is the AI-to-system semantic gap, which involves mapping the impacts of AI component-level attacks to system-level impacts.\\n2. In the context of autonomous driving (AD), an exponentially growing trend in the research to tackle the semantic AI security challenges has been noticed since 2019.\\n3. Existing surveys on AD security did not focus on the semantic AI security challenges and their impacts on AD AI components.\\n4. Given the safety-criticality of attacks in the AD context and the increase in research on the topic, the authors believe it is a good time to summarize the current status, trends, scientific gaps, insights, and future research directions.\\n5. In total, 53 papers related to semantic AD AI security are collected and analyzed. These papers have been classified based on critical aspects for the security field like targeted AI component, attack/defence goals, vector, knowledge, deployability, robustness, and evaluation methodologies.\\n6. Based on the systematization, six significant scientific gaps were identified. The authors suggest potential future directions not only at the design level but also at the research goal and methodology levels.',\n",
              "  \"The key points from this text include:\\n\\n1. The authors collect and evaluate research papers from top-tier venues and other well-known works in fields closely related to AD AI like security, Computer Vision (CV), Machine Learning (ML), AI, and robotics. They focus on research from 2017 to 2021.\\n2. Unlike previous reviews that focused on hardware and sensor security related to AD, this paper focuses on works involving semantic AD AI security, with an emphasis on research done later than 2019.\\n3. The authors look at the growth of semantic AD AI security work and argue that it forms an essential part of current research. Around 85% of this work has been developed since 2019.\\n4. The authors compare the AD AI research with similar security work on drones, automatic speech recognition and speaker identification, and sensor technology. \\n5. The authors examine 53 papers focusing on semantic AD AI security, of which 48 discovered new attacks, and 8 developed new defense solutions.\\n6. Initially, researchers dismissed the severity of adversarial attacks on AD vehicles, but later works have proven this wrong, demonstrating successful attacks on object detection functions in cars.\\n7. The authors aim to categorize (or 'systematize') the existing research efforts related to AD AI. This includes highlighting targeted AI components in existing works, status, and trends.\\n\"],\n",
              " 1: [\"1. In autonomous driving (AD), L1 vehicles have the AD system in control of either steering or throttling/braking, while L2 vehicles have partial automation, where the AD system controls both.\\n2. L1 and L2 vehicles still require active monitoring from the driver, but at levels L3 and above, driver attention is less necessary, and by levels L4 and L5, a driver seat is not required.\\n3. At L4 and L5 levels, the systems operate differently; L4 AD systems only operate in limited Operational Design Domains (ODDs), but L5 systems can handle all potential driving scenarios.\\n4. The AI components in AD systems include perception (understanding the surrounding environment), localization (finding the vehicle's position in the environment), prediction (estimating future statuses of surrounding objects), and planning (making driving decisions).\\n5. The modular design is the current industry standard for AD systems due to its easier debugging, interpretation, and ability to hard-code safety rules/measures.\\n6. Autonomous vehicles are susceptible to adversarial AI attacks, in which AI models are manipulated, raising concerns about system-level vulnerabilities.\\n7. The paper focuses on exploring semantic AI security within the context of autonomous driving, with a view to addressing AI component-level vulnerabilities.\",\n",
              "  '1. The general goal of Autonomous Driving (AD) AI stack is to achieve autonomy using onboard sensing, but there are networking channels that can affect end-to-end driving. \\n2. High Definition (HD) Map update channels are crucial for driving safety as they are integral for various AD modules such as localization, prediction, routing, and planning.\\n3. Autonomous vehicles at Level 4 (L4) and above, which do not have safety drivers onboard, often require a remote operator to take over control when a failsafe state is reached. This control channel is directly safety-critical if hijacked.\\n4. Another area of concern is the vast majority of attack works targeting the upstream AI components like perception and localization, and not focusing much on the downstream components like prediction and planning.\\n5. The downstream AI components are just as important, if not more, than the upstream ones. For instance, errors in obstacle trajectory prediction or path planning will directly affect driving decisions, leading to system-level effects. \\n6. In order to advance AD AI security research, there is a need to study the security properties of downstream AI components.\\n7. Possible solution directions include addressing the physical-layer attacks by manipulating road objects, affecting the inputs of prediction without relying on vulnerabilities in upstream components, and addressing physical-layer attacks by localization manipulation.'],\n",
              " 9: [\"1. Majority of existing works (>86%) focus on perception, while localization, chassis, and end-to-end driving receive less or equal to 6.2% attention.\\n2. The two most popular perception works are camera (60.0%) and LiDAR (21.5%) perception.\\n3. None of the current works study downstream AI components such as prediction and planning.\\n4. The semantic AD AI attacks are categorized based on 3 research aspects: Attack goal, attack vector, and attacker’s knowledge.\\n5. Attack goals are further divided based on integrity, confidentiality, and availability. \\n6. Integrity in AD context refers to the integrity of AI component outputs; its violations can lead to safety hazards, traffic rule violations, and mobility degradation.\\n7. Confidentiality is related to sensitive information from or collected by the AD vehicle.\\n8. Availability in the AD context can be defined as an AI component's ability to provide timely and reliable outputs; this can be affected through attacks causing delays or failures in the outputting function.\"],\n",
              " 7: [\"1. Most existing works on AI components in autonomous vehicle systems focus is on integrity (96.3%), with only 3.7% on confidentiality, and none on availability.\\n2. Attacks on autonomous driving systems can be categorized into two groups: physical-layer and cyber-layer attacks.\\n3. Physical-layer attacks involve tampering with the sensor inputs to the AI components physically, which can be further broken down into physical-world attacks and sensor attacks.\\n4. Cyber-layer attacks require internal access to the autonomous driving system, its computation platform, or even its development environment.\\n5. The object texture, in physical-world attack vectors, refers to changing the surface texture of 2D or 3D objects, which is frequently used in adversarial attacks.\\n6. Multiple studies have been conducted in the field of AD AI attacks, targeting various aspects such as AI component integrity, object detection, camera perception, semantic segmentation, object tracking, LiDAR detection, and RADAR perception, among others. \\n7. The understanding and execution of these attacks depend on the attacker's knowledge ranging from white-box knowledge (complete knowledge of the system), to gray-box (partial knowledge), and black-box (no knowledge).\\n8. The attack methods often involve patches, posters, and software compromises.\"],\n",
              " 5: [\"The text discusses various methods that hackers can use to attack autonomous driving (AD) systems. Some of these attacks aim at disguising objects or changing their appearance through camouflage or projectors. Examples include altering the appearance of stop signs, road surfaces, vehicles, and clothes. Other attacks focus on changing the shape or position of 3D objects, such as vehicles or traffic cones. Sensor attacks involve spoofing devices like LiDAR, RADAR, and GPS to cause them to produce false readings. Another attack vector is projecting laser or light directly onto the sensor to misguide object detection. Acoustic signals can also be used to disrupt the outputs of Inertial Measurement Units (IMUs). Hackers can also use ML backdoor methods and software compromises to manipulate the model's outputs or to infiltrate sensor data. The majority of attacks use physical-world attack vectors, with object texture alteration being the most common approach. Cyber-layer attack vectors are less commonly used in current hacking approaches.\"],\n",
              " 4: ['- The text discusses three attack settings on autonomous driving (AD) AI systems: white-box, gray-box, and black-box. In a white-box attack, the attacker has complete knowledge of the AD system; this is the most commonly adopted setting. Gray-box attacks assume that some information required for white-box attacks is unavailable, while black-box attacks are the most restrictive, with the attacker having no access to internal AD vehicle details. \\n- AD AI defense methods are categorized into two: consistency checking and adversarial robustness. Consistency checking cross-checks attacked information with other independent measurement sources or inherently unchanging properties of the information. Examples include using stereo cameras and prediction models to cross-check LiDAR object detection results, or checking if the current camera object detection results tally with the driving context. Adversarial robustness is another defense method but the text does not elaborate on this method.\\n- The field has been evolving, with more studies focusing on gray-box and black-box settings, which are more challenging but practical.\\n- Defense strategies are evaluated according to certain key factors: deployability, robustness to adaptive attacks, defense methods, and defense goals. The text provides a tabulated summary of various defense solutions, indicating whether they meet various evaluation criteria.'],\n",
              " 3: ['1. Robustness of the AI component against attacks is being improved through various defense methods such as adversarial training and predicting and removing potential adversarial perturbations.\\n2. There are two main defense goals; detection and mitigation. Detection-based defenses focus on detecting attack attempts while mitigation strategies aim to improve adversarial robustness to reduce attack success rate. \\n3. Defense methods should be designed for practical deployment, focusing on aspects such as negligible timing overhead, negligible resource overhead, no model training, no additional dataset requirement, and no hardware modification. Current defenses lack awareness in areas like no model training and negligible resource overhead.\\n4. Adaptive attacks, designed to circumvent specific defenses, have become a point of focus and calls for defenses that can withstand such attacks. They assume complete knowledge of the defense internals and directly challenge the fundamental assumptions of the defense. Existing defenses have yet to be thoroughly evaluated against such attacks.'],\n",
              " 0: [\"1. Adversarial AI defenses are strongly advocated and guidelines exist for designing adaptive attacks. \\n2. Only 3 of the relevant studies include evaluations of adaptive attack (Nassi et al., Liu et al., and Sun et al.). \\n3. Most existing defenses in AD AI security don't evaluate against adaptive attacks. \\n4. Expected differences in evaluation methodologies exist due to different problem formulations. \\n5. Evaluation methodology could be either at the level of the AI component or the AD system, involving both AI component and AD system.\\n6. Component-level evaluation looks at attack/defense impacts at the AI component level. \\n7. System-level evaluation considers the impacts at the vehicle driving behavior level and can be achieved via real vehicle-based or simulation-based setups. \\n8. Majority of surveyed works perform component-level evaluation while only about 25% adopt some form of system-level evaluation. \\n9. There is a call for attention to areas of identified scientific gaps in AD AI security and suggestions for future directions.\",\n",
              "  \"1. In AD (Autonomous Driving) systems, there is a lack of system-level evaluation, with only 25.4% of existing works conducting this evaluation; especially low numbers (7.4%) are seen in the component of camera object detection.\\n2. Most existing works (74.6%) only carry out component-level evaluations without determining the system-level effects of their attack/defense designs.\\n3. This lack of system-level evaluation is falling behind other CPS (Cyber-Physical Systems) domains like drones and ASR/SI (automated speech recognition/speaker identification), which often conduct system-level evaluations.\\n4. AD systems are particularly complex, and the system-level complexity and stringent control dynamics can create fault-tolerant effects for component-level errors.\\n5. High attack success rates at the component-level may not necessarily affect the AD vehicle's driving behavior.\\n6. The current lack of system-level evaluation in AD AI security research is a crucial scientific methodology-level gap that needs prompt attention.\\n7. Addressing this gap poses various technical challenges. Real vehicle-based system-level evaluation is generally unaffordable for most academic research groups and poses safety risks.\\n8. While simulation-based evaluation is more accessible and safe, it requires significant engineering efforts.\\n9. A potential solution could be a community-level effort to build a common system-level evaluation infrastructure, which could help address these challenges.\",\n",
              "  '1. The system-level impact of attacks in Automated Driving (AD) can be significantly affected by various driving scenario setups.\\n2. The system-level evaluation results can only be compared if the same evaluation scenario and metric calculation methods are used.\\n3. Defense solutions for AD AI security attacks are quite limited today, particularly those focusing on attack prevention.\\n4. Among existing AD AI security studies, 85.7% are focused on discovered attacks, while only 14.3% are focused on effective defense solutions.\\n5. There are several AD AI components with discovered attacks but no effective defense solutions yet, indicating concrete areas for future research.\\n6. Generic AI defenses are generally ineffective against AD AI attacks, therefore new defense strategies need to be explored.\\n7. Existing works mostly focus on physical-layer attack vectors, leaving cyber-layer attack vectors relatively unexplored, with only 11.1% of AD AI attack studies relating to cyber threats.\\n8. In related Critical Physical Systems (CPS) domains such as drones and ASR/SI, the focus on cyber-layer attack vectors is much higher (>50%).\\n9. There is a need to address the system-to-AI semantic gap and potential deployability challenges for the successful implementation of attack prevention.',\n",
              "  \"1. Changes in localization directly affect decision-making in driving path planning within autonomous driving (AD) systems. \\n2. Attack vectors, such as GPS spoofing, can be used to manipulate planning inputs, but this depends on the type of localization system in use.\\n3. Cyber attacks are direct methods to manipulate inputs in AD systems and can include attacking a ROS node to send malicious messages. \\n4. There's a focus on safety and rule violations in existing AD attacks, with less emphasis on other significant security aspects such as confidentiality, availability, and authenticity.\\n5. There's a lack of balanced study between different aspects of security in the AD domain, as compared to drone security, where integrity, privacy, and availability are more equally studied.\\n6. Future research should focus on exploring under-researched security properties, considering potential private information extraction from sensor inputs, and exploring more on cyber-layer attacks.\\n7. The openness of AD AI security works from the security community is lacking compared to other communities, which affects reproducibility and comparability.\\n8. Only a few papers from security conferences, specifically those related to sensor attacks, are open-source.\\n9. There's a need for more work on verifying the authenticity of safety drivers, passengers, and consumers in the AD AI stack.\"],\n",
              " 6: [\"1. About 50% of ASR/SI papers in CPS, CV, and ML/AI security conferences release their code, highlighting a willingness to share, but the diversity of AD AI security papers may limit this practice.\\n2. Security conference papers tend to use a diverse set of attack vectors, making code sharing difficult due to hardware design implications.\\n3. Papers in the ASR/SI domain primarily use malicious sound waves for attacks, which are easier to modify and evaluate digitally.\\n4. Encouraging open-sourcing efforts within the security community is beneficial for future research, although it is unclear how hardware implementations should be shared to benefit the community directly.\\n5. Two potential solutions include open-source hardware implementation references and open-source attack modeling code, with the former involving the release of detailed hardware design information and the latter involving the digital modelling of an attack's capability.\\n6. Open sourcing practices can be encouraged via community-level evaluation infrastructure development.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusterKeyToTheme = {}\n",
        "for cluster_key in clustered_sentences.keys():\n",
        "\n",
        "  allsentencesincluster = ' '.join(clustered_sentences[cluster_key])\n",
        "  response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f'What do the following key points have in common?\\n\\nKey points:\\n\"\"\"\\n{allsentencesincluster}\\n\"\"\"\\n\\nTheme:',\n",
        "        temperature=0,\n",
        "        max_tokens=64,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "  print('Cluster num', cluster_key, response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n",
        "  clusterKeyToTheme[cluster_key] = response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryMexOG2aHGx",
        "outputId": "bb2308e1-392d-4b10-c590-77a1d7073de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster num 8  Autonomous Driving (AD) AI security research and the development of an open source evaluation platform called PASS to address the gap in scientific methodology.\n",
            "Cluster num 2  The authors analyze research papers related to semantic AI security in the context of autonomous driving, and identify scientific gaps and potential future directions.\n",
            "Cluster num 1  The security of Autonomous Driving (AD) AI stack and the need to study the security properties of downstream AI components.\n",
            "Cluster num 9  Autonomous Driving AI Research and Security\n",
            "Cluster num 7  Attacks on Autonomous Driving Systems\n",
            "Cluster num 5 The common theme among the key points is the various methods of hacking autonomous driving (AD) systems.\n",
            "Cluster num 4  Autonomous Driving (AD) AI Systems\n",
            "Cluster num 3  Improving the robustness of AI components against attacks through defense methods.\n",
            "Cluster num 0  The lack of system-level evaluation and defense solutions in Autonomous Driving (AD) AI security research, and the need for more research in this area.\n",
            "Cluster num 6  The benefits of open-sourcing efforts within the security community, and potential solutions to encourage open sourcing practices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster_key in clustered_sentences.keys():\n",
        "  print(cluster_key)\n",
        "  print(clusterKeyToTheme[cluster_key])\n",
        "  print(clustered_sentences[cluster_key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdc5KmhPlrTH",
        "outputId": "6432fd0b-42fb-4d68-b4dc-c8ec3bc98585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            " Autonomous Driving (AD) AI security research and the development of an open source evaluation platform called PASS to address the gap in scientific methodology.\n",
            "['- Autonomous Driving (AD) systems rely on AI for safety and driving decisions, but these are vulnerable to adversarial attacks.\\n- \"Semantic AI security\" is the term referred to the research that explores the implications of these attacks not just at the component level, but at the system-wide level.\\n- Over the past five years, research in this field has grown and is now being performed to address these challenges in AD.\\n- This paper aims to systematically organise this emerging research field by analysing 53 papers. It categorises them based on aspects like the AI component targeted, the goal of the attack or defence, attack vectors, defence robustness, and evaluation methodologies.\\n- Six significant scientific gaps are identified and the paper provides potential future directions for research and design.\\n- An open source evaluation platform called PASS has been developed to address the gap in scientific methodology.\\n- While AI components are vulnerable to attacks, these don\\'t necessarily result in system-level vulnerabilities due to the \"semantic gaps\" between system-level attack input and AI component level.', 'This text discusses identified scientific gaps in autonomous vehicle (AD) AI security research, especially the general lack of system-level evaluation that could potentially lead to meaningless attack/defense progress. To fill this gap, the authors propose a community effort to build a common system-level evaluation infrastructure. They introduce their development of a uniform and extensible system-driven evaluation platform, named PASS, for the AD AI security research community, aiming to improve comparability, reproducibility, and to encourage open-sourcing. The platform has a simulation-centric hybrid design using both simulation and real vehicles. The authors also highlight the implementation of a prototype that system-level evaluates a popular AD AI attack category, proving the necessity of such an evaluation platform. The paper provides taxonomization of 53 related papers and analyses identifying the most substantial scientific gaps for future work.']\n",
            "2\n",
            " The authors analyze research papers related to semantic AI security in the context of autonomous driving, and identify scientific gaps and potential future directions.\n",
            "['1. The text discusses the concept of semantic AI security, which addresses two general semantic gaps in artificial intelligence (AI). One is the system-to-AI semantic gap, also called the inverse-feature mapping problem. The second is the AI-to-system semantic gap, which involves mapping the impacts of AI component-level attacks to system-level impacts.\\n2. In the context of autonomous driving (AD), an exponentially growing trend in the research to tackle the semantic AI security challenges has been noticed since 2019.\\n3. Existing surveys on AD security did not focus on the semantic AI security challenges and their impacts on AD AI components.\\n4. Given the safety-criticality of attacks in the AD context and the increase in research on the topic, the authors believe it is a good time to summarize the current status, trends, scientific gaps, insights, and future research directions.\\n5. In total, 53 papers related to semantic AD AI security are collected and analyzed. These papers have been classified based on critical aspects for the security field like targeted AI component, attack/defence goals, vector, knowledge, deployability, robustness, and evaluation methodologies.\\n6. Based on the systematization, six significant scientific gaps were identified. The authors suggest potential future directions not only at the design level but also at the research goal and methodology levels.', \"The key points from this text include:\\n\\n1. The authors collect and evaluate research papers from top-tier venues and other well-known works in fields closely related to AD AI like security, Computer Vision (CV), Machine Learning (ML), AI, and robotics. They focus on research from 2017 to 2021.\\n2. Unlike previous reviews that focused on hardware and sensor security related to AD, this paper focuses on works involving semantic AD AI security, with an emphasis on research done later than 2019.\\n3. The authors look at the growth of semantic AD AI security work and argue that it forms an essential part of current research. Around 85% of this work has been developed since 2019.\\n4. The authors compare the AD AI research with similar security work on drones, automatic speech recognition and speaker identification, and sensor technology. \\n5. The authors examine 53 papers focusing on semantic AD AI security, of which 48 discovered new attacks, and 8 developed new defense solutions.\\n6. Initially, researchers dismissed the severity of adversarial attacks on AD vehicles, but later works have proven this wrong, demonstrating successful attacks on object detection functions in cars.\\n7. The authors aim to categorize (or 'systematize') the existing research efforts related to AD AI. This includes highlighting targeted AI components in existing works, status, and trends.\\n\"]\n",
            "1\n",
            " The security of Autonomous Driving (AD) AI stack and the need to study the security properties of downstream AI components.\n",
            "[\"1. In autonomous driving (AD), L1 vehicles have the AD system in control of either steering or throttling/braking, while L2 vehicles have partial automation, where the AD system controls both.\\n2. L1 and L2 vehicles still require active monitoring from the driver, but at levels L3 and above, driver attention is less necessary, and by levels L4 and L5, a driver seat is not required.\\n3. At L4 and L5 levels, the systems operate differently; L4 AD systems only operate in limited Operational Design Domains (ODDs), but L5 systems can handle all potential driving scenarios.\\n4. The AI components in AD systems include perception (understanding the surrounding environment), localization (finding the vehicle's position in the environment), prediction (estimating future statuses of surrounding objects), and planning (making driving decisions).\\n5. The modular design is the current industry standard for AD systems due to its easier debugging, interpretation, and ability to hard-code safety rules/measures.\\n6. Autonomous vehicles are susceptible to adversarial AI attacks, in which AI models are manipulated, raising concerns about system-level vulnerabilities.\\n7. The paper focuses on exploring semantic AI security within the context of autonomous driving, with a view to addressing AI component-level vulnerabilities.\", '1. The general goal of Autonomous Driving (AD) AI stack is to achieve autonomy using onboard sensing, but there are networking channels that can affect end-to-end driving. \\n2. High Definition (HD) Map update channels are crucial for driving safety as they are integral for various AD modules such as localization, prediction, routing, and planning.\\n3. Autonomous vehicles at Level 4 (L4) and above, which do not have safety drivers onboard, often require a remote operator to take over control when a failsafe state is reached. This control channel is directly safety-critical if hijacked.\\n4. Another area of concern is the vast majority of attack works targeting the upstream AI components like perception and localization, and not focusing much on the downstream components like prediction and planning.\\n5. The downstream AI components are just as important, if not more, than the upstream ones. For instance, errors in obstacle trajectory prediction or path planning will directly affect driving decisions, leading to system-level effects. \\n6. In order to advance AD AI security research, there is a need to study the security properties of downstream AI components.\\n7. Possible solution directions include addressing the physical-layer attacks by manipulating road objects, affecting the inputs of prediction without relying on vulnerabilities in upstream components, and addressing physical-layer attacks by localization manipulation.']\n",
            "9\n",
            " Autonomous Driving AI Research and Security\n",
            "[\"1. Majority of existing works (>86%) focus on perception, while localization, chassis, and end-to-end driving receive less or equal to 6.2% attention.\\n2. The two most popular perception works are camera (60.0%) and LiDAR (21.5%) perception.\\n3. None of the current works study downstream AI components such as prediction and planning.\\n4. The semantic AD AI attacks are categorized based on 3 research aspects: Attack goal, attack vector, and attacker’s knowledge.\\n5. Attack goals are further divided based on integrity, confidentiality, and availability. \\n6. Integrity in AD context refers to the integrity of AI component outputs; its violations can lead to safety hazards, traffic rule violations, and mobility degradation.\\n7. Confidentiality is related to sensitive information from or collected by the AD vehicle.\\n8. Availability in the AD context can be defined as an AI component's ability to provide timely and reliable outputs; this can be affected through attacks causing delays or failures in the outputting function.\"]\n",
            "7\n",
            " Attacks on Autonomous Driving Systems\n",
            "[\"1. Most existing works on AI components in autonomous vehicle systems focus is on integrity (96.3%), with only 3.7% on confidentiality, and none on availability.\\n2. Attacks on autonomous driving systems can be categorized into two groups: physical-layer and cyber-layer attacks.\\n3. Physical-layer attacks involve tampering with the sensor inputs to the AI components physically, which can be further broken down into physical-world attacks and sensor attacks.\\n4. Cyber-layer attacks require internal access to the autonomous driving system, its computation platform, or even its development environment.\\n5. The object texture, in physical-world attack vectors, refers to changing the surface texture of 2D or 3D objects, which is frequently used in adversarial attacks.\\n6. Multiple studies have been conducted in the field of AD AI attacks, targeting various aspects such as AI component integrity, object detection, camera perception, semantic segmentation, object tracking, LiDAR detection, and RADAR perception, among others. \\n7. The understanding and execution of these attacks depend on the attacker's knowledge ranging from white-box knowledge (complete knowledge of the system), to gray-box (partial knowledge), and black-box (no knowledge).\\n8. The attack methods often involve patches, posters, and software compromises.\"]\n",
            "5\n",
            "The common theme among the key points is the various methods of hacking autonomous driving (AD) systems.\n",
            "[\"The text discusses various methods that hackers can use to attack autonomous driving (AD) systems. Some of these attacks aim at disguising objects or changing their appearance through camouflage or projectors. Examples include altering the appearance of stop signs, road surfaces, vehicles, and clothes. Other attacks focus on changing the shape or position of 3D objects, such as vehicles or traffic cones. Sensor attacks involve spoofing devices like LiDAR, RADAR, and GPS to cause them to produce false readings. Another attack vector is projecting laser or light directly onto the sensor to misguide object detection. Acoustic signals can also be used to disrupt the outputs of Inertial Measurement Units (IMUs). Hackers can also use ML backdoor methods and software compromises to manipulate the model's outputs or to infiltrate sensor data. The majority of attacks use physical-world attack vectors, with object texture alteration being the most common approach. Cyber-layer attack vectors are less commonly used in current hacking approaches.\"]\n",
            "4\n",
            " Autonomous Driving (AD) AI Systems\n",
            "['- The text discusses three attack settings on autonomous driving (AD) AI systems: white-box, gray-box, and black-box. In a white-box attack, the attacker has complete knowledge of the AD system; this is the most commonly adopted setting. Gray-box attacks assume that some information required for white-box attacks is unavailable, while black-box attacks are the most restrictive, with the attacker having no access to internal AD vehicle details. \\n- AD AI defense methods are categorized into two: consistency checking and adversarial robustness. Consistency checking cross-checks attacked information with other independent measurement sources or inherently unchanging properties of the information. Examples include using stereo cameras and prediction models to cross-check LiDAR object detection results, or checking if the current camera object detection results tally with the driving context. Adversarial robustness is another defense method but the text does not elaborate on this method.\\n- The field has been evolving, with more studies focusing on gray-box and black-box settings, which are more challenging but practical.\\n- Defense strategies are evaluated according to certain key factors: deployability, robustness to adaptive attacks, defense methods, and defense goals. The text provides a tabulated summary of various defense solutions, indicating whether they meet various evaluation criteria.']\n",
            "3\n",
            " Improving the robustness of AI components against attacks through defense methods.\n",
            "['1. Robustness of the AI component against attacks is being improved through various defense methods such as adversarial training and predicting and removing potential adversarial perturbations.\\n2. There are two main defense goals; detection and mitigation. Detection-based defenses focus on detecting attack attempts while mitigation strategies aim to improve adversarial robustness to reduce attack success rate. \\n3. Defense methods should be designed for practical deployment, focusing on aspects such as negligible timing overhead, negligible resource overhead, no model training, no additional dataset requirement, and no hardware modification. Current defenses lack awareness in areas like no model training and negligible resource overhead.\\n4. Adaptive attacks, designed to circumvent specific defenses, have become a point of focus and calls for defenses that can withstand such attacks. They assume complete knowledge of the defense internals and directly challenge the fundamental assumptions of the defense. Existing defenses have yet to be thoroughly evaluated against such attacks.']\n",
            "0\n",
            " The lack of system-level evaluation and defense solutions in Autonomous Driving (AD) AI security research, and the need for more research in this area.\n",
            "[\"1. Adversarial AI defenses are strongly advocated and guidelines exist for designing adaptive attacks. \\n2. Only 3 of the relevant studies include evaluations of adaptive attack (Nassi et al., Liu et al., and Sun et al.). \\n3. Most existing defenses in AD AI security don't evaluate against adaptive attacks. \\n4. Expected differences in evaluation methodologies exist due to different problem formulations. \\n5. Evaluation methodology could be either at the level of the AI component or the AD system, involving both AI component and AD system.\\n6. Component-level evaluation looks at attack/defense impacts at the AI component level. \\n7. System-level evaluation considers the impacts at the vehicle driving behavior level and can be achieved via real vehicle-based or simulation-based setups. \\n8. Majority of surveyed works perform component-level evaluation while only about 25% adopt some form of system-level evaluation. \\n9. There is a call for attention to areas of identified scientific gaps in AD AI security and suggestions for future directions.\", \"1. In AD (Autonomous Driving) systems, there is a lack of system-level evaluation, with only 25.4% of existing works conducting this evaluation; especially low numbers (7.4%) are seen in the component of camera object detection.\\n2. Most existing works (74.6%) only carry out component-level evaluations without determining the system-level effects of their attack/defense designs.\\n3. This lack of system-level evaluation is falling behind other CPS (Cyber-Physical Systems) domains like drones and ASR/SI (automated speech recognition/speaker identification), which often conduct system-level evaluations.\\n4. AD systems are particularly complex, and the system-level complexity and stringent control dynamics can create fault-tolerant effects for component-level errors.\\n5. High attack success rates at the component-level may not necessarily affect the AD vehicle's driving behavior.\\n6. The current lack of system-level evaluation in AD AI security research is a crucial scientific methodology-level gap that needs prompt attention.\\n7. Addressing this gap poses various technical challenges. Real vehicle-based system-level evaluation is generally unaffordable for most academic research groups and poses safety risks.\\n8. While simulation-based evaluation is more accessible and safe, it requires significant engineering efforts.\\n9. A potential solution could be a community-level effort to build a common system-level evaluation infrastructure, which could help address these challenges.\", '1. The system-level impact of attacks in Automated Driving (AD) can be significantly affected by various driving scenario setups.\\n2. The system-level evaluation results can only be compared if the same evaluation scenario and metric calculation methods are used.\\n3. Defense solutions for AD AI security attacks are quite limited today, particularly those focusing on attack prevention.\\n4. Among existing AD AI security studies, 85.7% are focused on discovered attacks, while only 14.3% are focused on effective defense solutions.\\n5. There are several AD AI components with discovered attacks but no effective defense solutions yet, indicating concrete areas for future research.\\n6. Generic AI defenses are generally ineffective against AD AI attacks, therefore new defense strategies need to be explored.\\n7. Existing works mostly focus on physical-layer attack vectors, leaving cyber-layer attack vectors relatively unexplored, with only 11.1% of AD AI attack studies relating to cyber threats.\\n8. In related Critical Physical Systems (CPS) domains such as drones and ASR/SI, the focus on cyber-layer attack vectors is much higher (>50%).\\n9. There is a need to address the system-to-AI semantic gap and potential deployability challenges for the successful implementation of attack prevention.', \"1. Changes in localization directly affect decision-making in driving path planning within autonomous driving (AD) systems. \\n2. Attack vectors, such as GPS spoofing, can be used to manipulate planning inputs, but this depends on the type of localization system in use.\\n3. Cyber attacks are direct methods to manipulate inputs in AD systems and can include attacking a ROS node to send malicious messages. \\n4. There's a focus on safety and rule violations in existing AD attacks, with less emphasis on other significant security aspects such as confidentiality, availability, and authenticity.\\n5. There's a lack of balanced study between different aspects of security in the AD domain, as compared to drone security, where integrity, privacy, and availability are more equally studied.\\n6. Future research should focus on exploring under-researched security properties, considering potential private information extraction from sensor inputs, and exploring more on cyber-layer attacks.\\n7. The openness of AD AI security works from the security community is lacking compared to other communities, which affects reproducibility and comparability.\\n8. Only a few papers from security conferences, specifically those related to sensor attacks, are open-source.\\n9. There's a need for more work on verifying the authenticity of safety drivers, passengers, and consumers in the AD AI stack.\"]\n",
            "6\n",
            " The benefits of open-sourcing efforts within the security community, and potential solutions to encourage open sourcing practices.\n",
            "[\"1. About 50% of ASR/SI papers in CPS, CV, and ML/AI security conferences release their code, highlighting a willingness to share, but the diversity of AD AI security papers may limit this practice.\\n2. Security conference papers tend to use a diverse set of attack vectors, making code sharing difficult due to hardware design implications.\\n3. Papers in the ASR/SI domain primarily use malicious sound waves for attacks, which are easier to modify and evaluate digitally.\\n4. Encouraging open-sourcing efforts within the security community is beneficial for future research, although it is unclear how hardware implementations should be shared to benefit the community directly.\\n5. Two potential solutions include open-source hardware implementation references and open-source attack modeling code, with the former involving the release of detailed hardware design information and the latter involving the digital modelling of an attack's capability.\\n6. Open sourcing practices can be encouraged via community-level evaluation infrastructure development.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusterKeyToTheme.values()"
      ],
      "metadata": {
        "id": "Qt7oel2SLqQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bf3046-fad5-44af-aa1d-e400650e2db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([' Autonomous Driving (AD) AI security research and the development of an open source evaluation platform called PASS to address the gap in scientific methodology.', ' The authors analyze research papers related to semantic AI security in the context of autonomous driving, and identify scientific gaps and potential future directions.', ' The security of Autonomous Driving (AD) AI stack and the need to study the security properties of downstream AI components.', ' Autonomous Driving AI Research and Security', ' Attacks on Autonomous Driving Systems', 'The common theme among the key points is the various methods of hacking autonomous driving (AD) systems.', ' Autonomous Driving (AD) AI Systems', ' Improving the robustness of AI components against attacks through defense methods.', ' The lack of system-level evaluation and defense solutions in Autonomous Driving (AD) AI security research, and the need for more research in this area.', ' The benefits of open-sourcing efforts within the security community, and potential solutions to encourage open sourcing practices.'])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusterKeyToTheme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q60yo8LAnxez",
        "outputId": "6cd5a993-575d-4fd2-e1af-bd02f2b218ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{8: ' Autonomous Driving (AD) AI security research and the development of an open source evaluation platform called PASS to address the gap in scientific methodology.',\n",
              " 2: ' The authors analyze research papers related to semantic AI security in the context of autonomous driving, and identify scientific gaps and potential future directions.',\n",
              " 1: ' The security of Autonomous Driving (AD) AI stack and the need to study the security properties of downstream AI components.',\n",
              " 9: ' Autonomous Driving AI Research and Security',\n",
              " 7: ' Attacks on Autonomous Driving Systems',\n",
              " 5: 'The common theme among the key points is the various methods of hacking autonomous driving (AD) systems.',\n",
              " 4: ' Autonomous Driving (AD) AI Systems',\n",
              " 3: ' Improving the robustness of AI components against attacks through defense methods.',\n",
              " 0: ' The lack of system-level evaluation and defense solutions in Autonomous Driving (AD) AI security research, and the need for more research in this area.',\n",
              " 6: ' The benefits of open-sourcing efforts within the security community, and potential solutions to encourage open sourcing practices.'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mb25fbHbpsC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}